{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40486e65-fc0b-4d4c-b4c8-f63cf6cf4afe",
   "metadata": {},
   "source": [
    "In this code, we load the well-known Iris dataset, which contains measurements for three iris flower species: Setosa, Versicolor, and Virginica. Each sample is represented by four features (sepal length, sepal width, petal length, and petal width). We then initialize and train a Decision Tree Classifier, setting a maximum depth of 3 to prevent overfitting and ensure interpretability. After training, this classifier can be used to predict the species of new iris samples based on the learned relationships in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30ca753-21df-4e31-9fa0-c9ec549a1cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, random_state=42)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "target_names = data.target_names\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f95ccb-4104-4059-a430-b2381c0ead69",
   "metadata": {},
   "source": [
    "In this code, we use the TreeSplanerClassifier class to interpret and convert the decision rules of a trained decision tree model into natural language. First, we initialize TreeSplanerClassifier with the classifier and provide feature and target names to enhance interpretability. The decision_tree_to_text() method generates a readable description of the decision rules, outlining conditions under which each class is predicted. We then use build_text_prediction() to explain the decision path and prediction details for specific sample inputs, making the predictions easier to understand. Lastly, branch_impurity() provides impurity metrics for each decision path, indicating the certainty of classifications at each node. This approach makes the inner workings of the decision tree more transparent and accessible for interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8b211d6-10c1-4ea0-8bc7-f3808c9df22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Rules in Natural Language:\n",
      " If  (petal length (cm) <= 2.45 and petal length (cm) <= -2.0 then class is setosa  with probability of 50.0 ) or (petal length (cm) > 2.45 and petal width (cm) > 1.75 and petal length (cm) <= 4.95 and petal length (cm) <= -2.0 then class is versicolor  with probability of 47.0 ) or (petal length (cm) > 2.45 and petal width (cm) > 1.75 and petal length (cm) <= 4.95 and petal length (cm) > -2.0 then class is virginica  with probability of 4.0 ) or (petal length (cm) > 2.45 and petal width (cm) > 1.75 and petal length (cm) > 4.85 and petal length (cm) <= -2.0 then class is virginica  with probability of 2.0 ) or (petal length (cm) > 2.45 and petal width (cm) > 1.75 and petal length (cm) > 4.85 and petal length (cm) > -2.0 then class is virginica  with probability of 43.0 )\n",
      "\n",
      "Sample Predictions:\n",
      " ['petal length (cm) <= 2.45 and petal length (cm) <= -2.0 therefore the predicted class is setosa with probability of 1.0', 'petal length (cm) <= 2.45 and petal width (cm) > 1.75 and petal length (cm) > 4.85 and petal length (cm) > -2.0 therefore the predicted class is virginica with probability of 1.0']\n",
      "\n",
      "Branch Impurity Information:\n",
      "  For branch 1  for split  0 and feature petal length (cm) impurity is 0.667  for split  1 and feature petal length (cm) impurity is 0.0  For branch 2  for split  0 and feature petal length (cm) impurity is 0.667  for split  2 and feature petal width (cm) impurity is 0.5  for split  3 and feature petal length (cm) impurity is 0.168  for split  4 and feature petal length (cm) impurity is 0.041  For branch 3  for split  0 and feature petal length (cm) impurity is 0.667  for split  2 and feature petal width (cm) impurity is 0.5  for split  3 and feature petal length (cm) impurity is 0.168  for split  5 and feature petal length (cm) impurity is 0.444  For branch 4  for split  0 and feature petal length (cm) impurity is 0.667  for split  2 and feature petal width (cm) impurity is 0.5  for split  6 and feature petal length (cm) impurity is 0.043  for split  7 and feature petal length (cm) impurity is 0.444  For branch 5  for split  0 and feature petal length (cm) impurity is 0.667  for split  2 and feature petal width (cm) impurity is 0.5  for split  6 and feature petal length (cm) impurity is 0.043  for split  8 and feature petal length (cm) impurity is 0.0 \n"
     ]
    }
   ],
   "source": [
    "from tree_splaner.tree_classifier_explainer import TreeSplanerClassifier\n",
    "\n",
    "# Initialize TreeSplanerClassifier with the trained classifier\n",
    "# Here we create an instance of TreeSplanerClassifier, which is designed to interpret\n",
    "# and convert a trained decision tree model's structure and predictions into natural language.\n",
    "# We pass the trained DecisionTreeClassifier instance, along with feature and target names for interpretability.\n",
    "tree_splaner = TreeSplanerClassifier(clf, feature_names=feature_names, target_names=target_names)\n",
    "\n",
    "# Convert the decision tree to a natural language description\n",
    "# This function generates a detailed, human-readable description of the decision rules\n",
    "# in the tree model, explaining the conditions under which each class would be predicted.\n",
    "decision_text = tree_splaner.decision_tree_to_text()\n",
    "print(\"Decision Tree Rules in Natural Language:\\n\", decision_text)\n",
    "\n",
    "# Generate text-based predictions for specific samples\n",
    "# This method builds a natural language explanation of predictions for given samples,\n",
    "# showing the specific path each sample would take through the decision tree.\n",
    "sample_predictions = tree_splaner.build_text_prediction(samples=[[5.1, 3.5, 1.4, 0.2], [6.3, 3.3, 6.0, 2.5]])\n",
    "print(\"\\nSample Predictions:\\n\", sample_predictions)\n",
    "\n",
    "# Display branch impurity information\n",
    "# This function outputs impurity metrics for each decision path in the tree, giving insights\n",
    "# into how \"pure\" each branch is, which reflects the certainty of classifications at each node.\n",
    "branch_impurity_info = tree_splaner.branch_impurity()\n",
    "print(\"\\nBranch Impurity Information:\\n\", branch_impurity_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028ad60-5f03-426d-9715-e564f45d1947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
